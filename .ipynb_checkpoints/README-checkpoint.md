# Welcome to all of the mini pet projects that I've done over time!

## Wordle Solver
With the craze of Wordle, I also jumped onto the Wordle train and wanted to see if I could create an optimal algorithm to solve the words of the day quicker than me! It takes in all of the possible 5 letter words and parses through the most frequent combinations and guesses the word with the most frequent occurances. The user inputs the word of the day and then runs the function that guesses for the word until it arrives at the word of the day. The algo is trained also on the clues (gray, yellow, green) to help determine the word of the day.

## Feature Selection Tutorial
In building models, choosing the right features is key to building not only efficient but also explainable models. Here, we'll dive into using XGBoost to perform feature selection. XGBoost can be leveraged beyond it's predictive powers it also helps us identify and select the most important features automatically. In this notebook, we’ll explore how XGBoost can simplify our models by focusing on the features that truly matter, improving both performance and efficiency!

## Doubly Robust Estimation Tutorial  
In this notebook, we explore **Doubly Robust Estimation** to estimate the **Average Treatment Effect (ATE)** in observational studies. This method combines two models—one for predicting treatment assignment (propensity scores) and one for predicting outcomes—to correct for biases in non-randomized data. We also use bootstrapping to calculate a **95% confidence interval** for the ATE. This tutorial walks through the steps to apply the method in Python and interpret the results, helping you get more reliable treatment effect estimates in observational settings.
